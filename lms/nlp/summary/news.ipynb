{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37409</th>\n",
       "      <td>The title says it all! These butter biscuits a...</td>\n",
       "      <td>The Aroma! The Taste!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41489</th>\n",
       "      <td>My dog has food allergies so it's hard to find...</td>\n",
       "      <td>My dog loves these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>This stuff really works, makes lemons taste sw...</td>\n",
       "      <td>Really works!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>I thought this would be a great decaf option, ...</td>\n",
       "      <td>Decaf Option.. Mild Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16381</th>\n",
       "      <td>THIS HAS BEEN MY FAVORITE COFFEE SINCE DAY ONE...</td>\n",
       "      <td>LOVE THIS COFFEE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25226</th>\n",
       "      <td>I've been eating more healthy rice dishes late...</td>\n",
       "      <td>Sanskriti's working 4 me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>I bought Fireworks Wisconsin white birch popco...</td>\n",
       "      <td>Absolutely Perfect - Just don't heat at too hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70558</th>\n",
       "      <td>I love any good peanut butter cookie and thoug...</td>\n",
       "      <td>These cookies are TERRIBLE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39151</th>\n",
       "      <td>I have an 11 pound Couton De Tulear.  Small sh...</td>\n",
       "      <td>Dog really likes, but they go fast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26865</th>\n",
       "      <td>my little boy loves this meal, he has been abl...</td>\n",
       "      <td>no preservatives and fast to make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80738</th>\n",
       "      <td>I have been using these herbs on everything.  ...</td>\n",
       "      <td>Great flavor!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89560</th>\n",
       "      <td>On the back of the package there's the questio...</td>\n",
       "      <td>Especially good if your dog is a Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47124</th>\n",
       "      <td>I love the flavor of this tea. I subscribed to...</td>\n",
       "      <td>Is Frequent Use Bad for Your Teeth?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83384</th>\n",
       "      <td>I was a little hesitant to try these, but even...</td>\n",
       "      <td>Excellent taste &amp; deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31672</th>\n",
       "      <td>I used to use the cheaper grocery store brands...</td>\n",
       "      <td>excellent dry cat food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "37409  The title says it all! These butter biscuits a...   \n",
       "41489  My dog has food allergies so it's hard to find...   \n",
       "9729   This stuff really works, makes lemons taste sw...   \n",
       "4505   I thought this would be a great decaf option, ...   \n",
       "16381  THIS HAS BEEN MY FAVORITE COFFEE SINCE DAY ONE...   \n",
       "25226  I've been eating more healthy rice dishes late...   \n",
       "10576  I bought Fireworks Wisconsin white birch popco...   \n",
       "70558  I love any good peanut butter cookie and thoug...   \n",
       "39151  I have an 11 pound Couton De Tulear.  Small sh...   \n",
       "26865  my little boy loves this meal, he has been abl...   \n",
       "80738  I have been using these herbs on everything.  ...   \n",
       "89560  On the back of the package there's the questio...   \n",
       "47124  I love the flavor of this tea. I subscribed to...   \n",
       "83384  I was a little hesitant to try these, but even...   \n",
       "31672  I used to use the cheaper grocery store brands...   \n",
       "\n",
       "                                                 Summary  \n",
       "37409                              The Aroma! The Taste!  \n",
       "41489                                 My dog loves these  \n",
       "9729                                       Really works!  \n",
       "4505                          Decaf Option.. Mild Flavor  \n",
       "16381                                  LOVE THIS COFFEE!  \n",
       "25226                           Sanskriti's working 4 me  \n",
       "10576  Absolutely Perfect - Just don't heat at too hi...  \n",
       "70558                        These cookies are TERRIBLE!  \n",
       "39151                Dog really likes, but they go fast.  \n",
       "26865                  no preservatives and fast to make  \n",
       "80738                                     Great flavor!!  \n",
       "89560        Especially good if your dog is a Vegetarian  \n",
       "47124                Is Frequent Use Bad for Your Teeth?  \n",
       "83384                             Excellent taste & deal  \n",
       "31672                             excellent dry cat food  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  everything bought great infact ordered twice third ordered wasfor mother father\n",
      "summary: great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88425/88425 [10:07<00:00, 145.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in tqdm(data['Text']):\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88425/88425 [00:15<00:00, 5724.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 전처리 후 결과:  ['good quality dog food', 'not as advertised', 'delight says it all', 'cough medicine', 'great taffy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in tqdm(data['Summary']):\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcwUlEQVR4nO3dfbgWdb3v8fdHUHT7BARxIZgLj5x29qAhKl1ZWe4QH3baOWp6LNBIrtLS9q4Mtp18KK/0tI+W7VIp2aLbNE5mchRDQsjdKRVQEvBhs0Tcgg+gKKCWCX7PH/O7ZViuh2Fg7nvda31e1zXXmvnOb+b+zrplfZ2Z3/xGEYGZmVkZOzU6ATMza14uImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiVhFJr+SmNyX9Obd8eon9HSlpVRW5mpXVt9EJmPVUEbFHbV7SSuALEfHbxmVktuP5TMSsziTtJGmypCckvShphqSBad3Vkm7Ntb1c0lxJuwN3Afvkzmb2adQxmNW4iJjV31eAE4GPAfsALwE/Tuu+Brxf0hmSPgJMBCZExKvAMcAzEbFHmp6pf+pmW/PlLLP6+yLw5YhYBSDpIuA/JX0uIl6T9Dmys46NwFdq7cy6IxcRs/rbD7hN0pu52GZgCLA6Iu6XtAJ4JzCjEQmaFeXLWWb19zRwTET0z027RsRqAEnnAP2AZ4Dzc9t5yG3rdlxEzOrvGuBSSfsBSBos6YQ0/1+B7wKfBT4HnC/p4LTd88A7JO1d/5TN2uciYlZ/PwRmAndL2gjcBxwuqS/wb8DlEfGniFgO/BNwo6R+EfEYcDOwQtLL7p1l3YH8UiozMyvLZyJmZlaai4iZmZXmImJmZqW5iJiZWWm97mHDQYMGRUtLS6PTMDNrGosWLXohIga3t67XFZGWlhYWLlzY6DTMzJqGpKc6WufLWWZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlZar3tifXu0TL6zw3UrLzuujpmYmXUPPhMxM7PSKi0iklZKWiJpsaSFKTZQ0hxJy9PPASkuSVdJapX0sKRRuf1MSO2XS5qQix+S9t+atlWVx2NmZlurx5nIxyPi4IgYnZYnA3MjYiQwNy0DHAOMTNMk4GrIig5wIXA4cBhwYa3wpDZn5bYbV/3hmJlZTSMuZ50ATE/z04ETc/EbInMf0F/SUOBoYE5ErIuIl4A5wLi0bq+IuC+yF8XfkNuXmZnVQdVFJIC7JS2SNCnFhkTEs2n+OWBImh8GPJ3bdlWKdRZf1U78bSRNkrRQ0sK1a9duz/GYmVlO1b2zjoiI1ZLeCcyR9Fh+ZUSEpKg4ByJiKjAVYPTo0ZV/nplZb1HpmUhErE4/1wC3kd3TeD5diiL9XJOarwb2zW0+PMU6iw9vJ25mZnVSWRGRtLukPWvzwFhgKTATqPWwmgDcnuZnAuNTL60xwPp02Ws2MFbSgHRDfSwwO63bIGlM6pU1PrcvMzOrgyovZw0Bbku9bvsCP4+I30haAMyQNBF4CjgltZ8FHAu0Aq8BZwJExDpJ3wEWpHaXRMS6NH82cD2wG3BXmszMrE4qKyIRsQI4qJ34i8BR7cQDOKeDfU0DprUTXwi8b7uTNTOzUvzEupmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlplRcRSX0kPSTpjrQ8QtL9klol/ULSLineLy23pvUtuX1MSfHHJR2di49LsVZJk6s+FjMz21o9zkTOAx7NLV8OXBkRBwAvARNTfCLwUopfmdoh6UDgVOC9wDjgJ6kw9QF+DBwDHAicltqamVmdVFpEJA0HjgN+lpYFfAL4ZWoyHTgxzZ+Qlknrj0rtTwBuiYjXI+JJoBU4LE2tEbEiIv4K3JLamplZnVR9JvID4HzgzbT8DuDliNiUllcBw9L8MOBpgLR+fWr/VrzNNh3F30bSJEkLJS1cu3btdh6SmZnVVFZEJB0PrImIRVV9RlERMTUiRkfE6MGDBzc6HTOzHqNvhfv+MPApSccCuwJ7AT8E+kvqm842hgOrU/vVwL7AKkl9gb2BF3Pxmvw2HcXNzKwOKjsTiYgpETE8IlrIbozfExGnA/OAk1KzCcDtaX5mWiatvyciIsVPTb23RgAjgQeABcDI1Ntrl/QZM6s6HjMze7sqz0Q68k3gFknfBR4Crkvx64AbJbUC68iKAhGxTNIM4BFgE3BORGwGkPRlYDbQB5gWEcvqeiRmZr1cXYpIRMwH5qf5FWQ9q9q2+QtwcgfbXwpc2k58FjBrB6ZqZmbbwE+sm5lZaV0WEUknS9ozzX9L0q8kjao+NTMz6+6KnIn8z4jYKOkI4O/I7l1cXW1aZmbWDIoUkc3p53HA1Ii4E9ilupTMzKxZFCkiqyVdC3wGmCWpX8HtzMyshytSDE4h60Z7dES8DAwEvlFlUmZm1hy6LCIR8RqwBjgihTYBy6tMyszMmkOR3lkXkj0gOCWFdgb+rcqkzMysORS5nPVp4FPAqwAR8QywZ5VJmZlZcyhSRP6axrAKAEm7V5uSmZk1iyJFZEbqndVf0lnAb4GfVpuWmZk1gy7HzoqIf5b0SWAD8G7g2xExp/LMzMys2ys0AGMqGi4cZma2lQ6LiKSNpPsgbVcBERF7VZaVmZk1hQ6LSES4B5aZmXWq0OWsNGrvEWRnJr+PiIcqzcrMzJpCkYcNvw1MB94BDAKul/StqhMzM7Pur8iZyOnAQenNg0i6DFgMfLfCvMzMrAkUeU7kGWDX3HI/YHU16ZiZWTMpciayHlgmaQ7ZPZFPAg9IugogIs6tMD8zM+vGihSR29JUM7+aVMzMrNkUeWJ9ej0SMTOz5lOkd9bxkh6StE7SBkkbJW2oR3JmZta9Fbmc9QPgvwFL0mi+ZmZmQLHeWU8DS11AzMysrSJnIucDsyT9Dni9FoyIKyrLyszMmkKRInIp8ArZsyK7VJuOmZk1kyJFZJ+IeF/lmZiZWdMpck9klqSxlWdiZmZNp0gR+RLwG0l/dhdfMzPLK/Kwod8rYmZm7Sr6PpEBwEhyAzFGxL1VJWVmZs2hyBPrXwDuBWYDF6efFxXYbldJD0j6k6Rlki5O8RGS7pfUKukXknZJ8X5puTWtb8nta0qKPy7p6Fx8XIq1Spq8jcduZmbbqcg9kfOAQ4GnIuLjwAeBlwts9zrwiYg4CDgYGCdpDHA5cGVEHAC8BExM7ScCL6X4lakdkg4ETgXeC4wDfiKpj6Q+wI+BY4ADgdNSWzMzq5MiReQvuRdS9YuIx4B3d7VRZF5JizunKYBPAL9M8enAiWn+hLRMWn+UJKX4LRHxekQ8CbQCh6WpNSJWRMRfgVtSWzMzq5MiRWSVpP7Ar4E5km4Hniqy83TGsBhYA8wBngBejohNtX0Dw9L8MLIhVkjr15O9kveteJttOoq3l8ckSQslLVy7dm2R1M3MrIAivbM+nWYvkjQP2Bv4TZGdR8Rm4OBUhG4D/rZkntslIqYCUwFGjx7tMcDMzHaQIjfW/4ukfrVFoAX4m235kIh4GZgHfAjoL6lWvIaz5VW7q4F902f2JStWL+bjbbbpKG5mZnVS5HLWrcBmSQeQ/d/8vsDPu9pI0uB0BoKk3cheq/soWTE5KTWbANye5memZdL6e9LIwTOBU1PvrRFkXY0fABYAI1Nvr13Ibr7PLHA8Zma2gxR5TuTNiNgk6dPAjyLiR5IeKrDdUGB66kW1EzAjIu6Q9Ahwi6TvAg8B16X21wE3SmoF1pEVBSJimaQZwCPAJuCcdJkMSV8m63LcB5gWEcsKHreZme0ARYrIG5JOIztL+PsU27mrjSLiYbLuwG3jK8h6VrWN/wU4uYN9XUo2mnDb+CxgVle5mJlZNYpczjqT7F7GpRHxZLqkdGO1aZmZWTMo0jvrEeDc3PKTpAcBzcysdytyJmJmZtYuFxEzMyutwyIi6cb087z6pWNmZs2kszORQyTtA3xe0gBJA/NTvRI0M7Puq7Mb69cAc4H9gUVkT6vXRIqbmVkv1uGZSERcFRHvIXuIb/+IGJGbXEDMzKxQF98vSToI+EgK3ZseJDQzs16uyACM5wI3Ae9M002SvlJ1YmZm1v0VGfbkC8DhEfEqgKTLgT8CP6oyMTMz6/6KPCciYHNueTNb32Q3M7NeqsiZyL8C90u6LS2fyJaRd83MrBcrcmP9CknzgSNS6MyIKDIUvJmZ9XBFzkSIiAeBByvOxczMmozHzjIzs9JcRMzMrLROi4ikPpLm1SsZMzNrLp0WkfQu8zcl7V2nfMzMrIkUubH+CrBE0hzg1VowIs7teJPep2XynZ2uX3nZcXXKxMysfooUkV+lyczMbCtFnhOZLmk34F0R8XgdcjIzsyZRZADGvwcWA79JywdLmllxXmZm1gSKdPG9CDgMeBkgIhbjF1KZmRnFisgbEbG+TezNKpIxM7PmUuTG+jJJ/wPoI2kkcC7wh2rTMjOzZlDkTOQrwHuB14GbgQ3AVyvMyczMmkSR3lmvARekl1FFRGysPi0zM2sGRXpnHSppCfAw2UOHf5J0SPWpmZlZd1fknsh1wNkR8e8Ako4ge1HVB6pMzMzMur8i90Q21woIQET8HthUXUpmZtYsOiwikkZJGgX8TtK1ko6U9DFJPwHmd7VjSftKmifpEUnLJJ2X4gMlzZG0PP0ckOKSdJWkVkkPp8+u7WtCar9c0oRc/BBJS9I2V0nyu9/NzOqos8tZ/7vN8oW5+Siw703A1yLiQUl7AovSII5nAHMj4jJJk4HJwDeBY4CRaTocuBo4XNLA9Nmj0+cukjQzIl5Kbc4C7gdmAeOAuwrkZmZmO0CHRSQiPr49O46IZ4Fn0/xGSY8Cw4ATgCNTs+lkZzXfTPEbIiKA+yT1lzQ0tZ0TEesAUiEal977vldE3JfiNwAn4iJiZlY3Xd5Yl9QfGA+05Ntvy1DwklqAD5KdMQxJBQbgOWBImh8GPJ3bbFWKdRZf1U68vc+fBEwCeNe73lU0bTMz60KR3lmzgPuAJZQY7kTSHsCtwFcjYkP+tkVEhKQil8a2S0RMBaYCjB49uvLPMzPrLYoUkV0j4h/L7FzSzmQF5KaIqL2T5HlJQyPi2XS5ak2Krwb2zW0+PMVWs+XyVy0+P8WHt9PezMzqpEgX3xslnSVpaOpZNTDd7O5U6il1HfBoRFyRWzUTqPWwmgDcnouPT720xgDr02Wv2cBYSQNST66xwOy0boOkMemzxuf2ZWZmdVDkTOSvwPeBC9jSKyvoejj4DwOfI3vKfXGK/RNwGTBD0kTgKeCUtG4WcCzQCrwGnAkQEeskfQdYkNpdUrvJDpwNXA/sRnZD3TfVzczqqEgR+RpwQES8sC07Tg8ldvTcxlHttA/gnA72NQ2Y1k58IfC+bcnLzMx2nCKXs2pnBmZmZlspcibyKrBY0jyy4eCBbevia2ZmPVORIvLrNJmZmW2lyPtEptcjETMzaz5Fnlh/knbGyoqIrnpnmZlZD1fkctbo3PyuwMlAl8+JmJlZz9dl76yIeDE3rY6IHwDHVZ+amZl1d0UuZ43KLe5EdmZS5AzGzMx6uCLFIP9ekU3ASrY8ZW5mZr1Ykd5Z2/VeETMz67mKXM7qB/x33v4+kUuqS8vMzJpBkctZtwPrgUXknlg3MzMrUkSGR8S4yjMxM7OmU2QAxj9Ien/lmZiZWdMpciZyBHBGenL9dbLh3SMiPlBpZmZm1u0VKSLHVJ6FmZk1pSJdfJ+qRyJmZtZ8itwTMTMza5eLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZlVZZEZE0TdIaSUtzsYGS5khann4OSHFJukpSq6SHJY3KbTMhtV8uaUIufoikJWmbqySpqmMxM7P2VXkmcj3Q9t3sk4G5ETESmJuWIXvx1cg0TQKuhqzoABcChwOHARfWCk9qc1ZuO78H3sysziorIhFxL7CuTfgEYHqanw6cmIvfEJn7gP6ShgJHA3MiYl1EvATMAcaldXtFxH0REcANuX2ZmVmd1PueyJCIeDbNPwcMSfPDgKdz7ValWGfxVe3E2yVpkqSFkhauXbt2+47AzMze0rAb6+kMIur0WVMjYnREjB48eHA9PtLMrFeodxF5Pl2KIv1ck+KrgX1z7YanWGfx4e3EzcysjupdRGYCtR5WE4Dbc/HxqZfWGGB9uuw1GxgraUC6oT4WmJ3WbZA0JvXKGp/bl5mZ1UnfqnYs6WbgSGCQpFVkvawuA2ZImgg8BZySms8CjgVagdeAMwEiYp2k7wALUrtLIqJ2s/5ssh5guwF3pcnMzOqosiISEad1sOqodtoGcE4H+5kGTGsnvhB43/bkaGZm28dPrJuZWWkuImZmVpqLiJmZleYiYmZmpVV2Y9221jL5zk7Xr7zsuDplYma24/hMxMzMSnMRMTOz0lxEzMysNBcRMzMrzUXEzMxKcxExM7PSXETMzKw0FxEzMyvNRcTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0lxEzMysNBcRMzMrzUXEzMxKcxExM7PS/HrcbqKz1+f61blm1l35TMTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0tzFtwl01v0X3AXYzBrHZyJmZlZa05+JSBoH/BDoA/wsIi5rcEp15wcVzaxRmrqISOoD/Bj4JLAKWCBpZkQ80tjMug9fCjOzKjV1EQEOA1ojYgWApFuAEwAXkYK6KjKdcQEys2YvIsOAp3PLq4DD2zaSNAmYlBZfkfR4ic8aBLxQYrvuZocdhy7fEXsppSd8Fz3hGKBnHEdPOAao9jj262hFsxeRQiJiKjB1e/YhaWFEjN5BKTVMTzgOH0P30ROOoyccAzTuOJq9d9ZqYN/c8vAUMzOzOmj2IrIAGClphKRdgFOBmQ3Oycys12jqy1kRsUnSl4HZZF18p0XEsoo+brsuh3UjPeE4fAzdR084jp5wDNCg41BENOJzzcysB2j2y1lmZtZALiJmZlaai0gBksZJelxSq6TJjc6nI5L2lTRP0iOSlkk6L8UHSpojaXn6OSDFJemqdFwPSxrV2CPYQlIfSQ9JuiMtj5B0f8r1F6kjBZL6peXWtL6loYnnSOov6ZeSHpP0qKQPNdt3Iekf0n9LSyXdLGnXZvguJE2TtEbS0lxsm3/3kiak9sslTegGx/D99N/Tw5Juk9Q/t25KOobHJR2di1f79ysiPHUykd2wfwLYH9gF+BNwYKPz6iDXocCoNL8n8B/AgcD/Aian+GTg8jR/LHAXIGAMcH+jjyF3LP8I/By4Iy3PAE5N89cAX0rzZwPXpPlTgV80OvfcMUwHvpDmdwH6N9N3QfYw75PAbrnv4Ixm+C6AjwKjgKW52Db97oGBwIr0c0CaH9DgYxgL9E3zl+eO4cD0t6kfMCL9zepTj79fDf2PtBkm4EPA7NzyFGBKo/MqmPvtZOOKPQ4MTbGhwONp/lrgtFz7t9o1OO/hwFzgE8Ad6R/3C7l/PG99J2Q98z6U5vumduoGx7B3+gOsNvGm+S7YMiLEwPS7vQM4ulm+C6ClzR/gbfrdA6cB1+biW7VrxDG0Wfdp4KY0v9Xfpdp3UY+/X76c1bX2hlYZ1qBcCkuXEj4I3A8MiYhn06rngCFpvrse2w+A84E30/I7gJcjYlNazuf51jGk9etT+0YbAawF/jVdlvuZpN1pou8iIlYD/wz8J/As2e92Ec33XdRs6+++230nbXye7AwKGngMLiI9kKQ9gFuBr0bEhvy6yP53pNv265Z0PLAmIhY1Opft1JfsUsTVEfFB4FWySyhvaYLvYgDZgKYjgH2A3YFxDU1qB+nuv/uuSLoA2ATc1OhcXES61lRDq0jamayA3BQRv0rh5yUNTeuHAmtSvDse24eBT0laCdxCdknrh0B/SbWHY/N5vnUMaf3ewIv1TLgDq4BVEXF/Wv4lWVFppu/i74AnI2JtRLwB/Irs+2m276JmW3/33fE7QdIZwPHA6akYQgOPwUWka00ztIokAdcBj0bEFblVM4Faz5IJZPdKavHxqXfKGGB97nS/ISJiSkQMj4gWst/1PRFxOjAPOCk1a3sMtWM7KbVv+P9hRsRzwNOS3p1CR5G9oqBpvguyy1hjJP1N+m+rdgxN9V3kbOvvfjYwVtKAdFY2NsUaRtlL+M4HPhURr+VWzQROTT3kRgAjgQeox9+vet4kataJrPfGf5D1crig0fl0kucRZKfoDwOL03Qs2XXpucBy4LfAwNReZC/1egJYAoxu9DG0OZ4j2dI7a//0j6IV+D9AvxTfNS23pvX7NzrvXP4HAwvT9/Frsh4+TfVdABcDjwFLgRvJev90++8CuJnsPs4bZGeFE8v87snuO7Sm6cxucAytZPc4av++r8m1vyAdw+PAMbl4pX+/POyJmZmV5stZZmZWmouImZmV5iJiZmaluYiYmVlpLiJmZlaai4j1WJJeqWCfB0s6Nrd8kaSvb8f+Tk4j/M7bMRmWzmOlpEGNzMGak4uI2bY5mKzf/Y4yETgrIj6+A/dpVjcuItYrSPqGpAXpPQwXp1hLOgv4aXpnxt2SdkvrDk1tF6d3OCxNT/xeAnwmxT+Tdn+gpPmSVkg6t4PPP03SkrSfy1Ps22QPiF4n6ftt2g+VdG/6nKWSPpLiV0tamPK9ONd+paTvpfYLJY2SNFvSE5K+mNocmfZ5Z3q/xDWS3vY3QNJnJT2Q9nWtsne79JF0fcpliaR/2M6vxHqKRj8R68lTVRPwSvo5FphK9mTyTmRDmn+UbJjtTcDBqd0M4LNpfilbhjW/jDQcN9n7NP4l9xkXAX8ge5J7ENlYUTu3yWMfsiFEBpMNzHgPcGJaN592nk4HvkZ6upjsnRB7pvmBudh84ANpeSVb3utxJdlT8numz3w+xY8E/kL2xHkfYA5wUm77QcB7gP9bOwbgJ8B44BBgTi6//o3+fj11j8lnItYbjE3TQ8CDwN+SjS0E2QCDi9P8IqBF2dvi9oyIP6b4z7vY/50R8XpEvEA2qN+QNusPBeZHNpBhbeTVj3axzwXAmZIuAt4fERtT/BRJD6ZjeS/Zy4hqamMiLSF7sdLGiFgLvK4tb8B7ICJWRMRmsmE1jmjzuUeRFYwFkhan5f3JXsi0v6QfpfGbNmBG9n9FZj2dgO9FxLVbBbN3rryeC20Gdiux/7b72O5/VxFxr6SPAscB10u6Avh34OvAoRHxkqTrycarapvHm21yejOXU9txjtouC5geEVPa5iTpILKXUn0ROIVsXCnr5XwmYr3BbODzyt6zgqRhkt7ZUeOIeBnYKOnwFDo1t3oj2WWibfEA8DFJgyT1IXtj3u8620DSfmSXoX4K/IxsGPm9yN5Lsl7SEOCYbcwD4LA0outOwGeA37dZPxc4qfb7UfZe8v1Sz62dIuJW4FspHzOfiVjPFxF3S3oP8MdsRHNeAT5LdtbQkYnATyW9SfYHf32KzwMmp0s93yv4+c9Kmpy2Fdnlr9u72OxI4BuS3kj5jo+IJyU9RDaq7tPA/yvy+W0sAP4FOCDlc1ubXB+R9C3g7lRo3gDOAf5M9pbG2v94vu1MxXonj+Jr1g5Je0TEK2l+Mtm7uc9rcFrbRdKRwNcj4vgGp2I9iM9EzNp3nKQpZP9GniLrlWVmbfhMxMzMSvONdTMzK81FxMzMSnMRMTOz0lxEzMysNBcRMzMr7f8Do1dsKbWvtPIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgM0lEQVR4nO3de7hVdb3v8fdHUrPShCQOcmmhkWXuQl1e9rPJaLtV1E7oPmXQSdBMMjXtZCVWJ90WT3SzNruyMEksL7G3mmzFkDya3VQWyuHiJZaIR9gIJCp4iQS/54/xWzpcrLUYjLXmnMw5P6/nmc8c4ztu3+F8WF/H+P3GbygiMDMzK2OXWidgZmb1y0XEzMxKcxExM7PSXETMzKw0FxEzMyvNRcTMzEpzETEzs9JcRMx6IGm0pD9KelbSBkl/kHRYrfMy21m8rtYJmO2sJO0F3AJ8GpgN7Aa8D9hcy7x2hCQBioiXa52LNSZfiZh17x0AEXFdRGyNiBcj4vaIWCzpEkm/6FhRUoukkPS6NH+XpK+nq5jnJP2npLdIukbSRkkLJLXktg9JZ0taLmmTpK9J2j9tv1HSbEm7pXX7S7pF0npJT6fpobl93SVpqqQ/AC8AF0hamD8xSZ+TdHNF/+tZU3ARMeven4GtkmZJOl5S/x3cfjxwKjAE2B/4E/AzYADwEHBxp/WPAw4FjgS+CMwAPg4MAw4CJqT1dkn7eRswHHgR+EGnfZ0KTAb2BKYDIyS9q9Pyq3fwfMy24SJi1o2I2AiMBgK4AlgvaY6kQQV38bOIeDQingVuAx6NiN9ExBbg34GDO63/rYjYGBHLgKXA7RGxIrf9wSmvpyLihoh4ISI2AVOB93fa11URsSwitkTEZuCXZAUJSe8GWshu1Zn1iouIWQ8i4qGIOC0ihpJdDewLfL/g5mtz0y92Mf+mMutLeoOkn0h6XNJG4G5gb0n9cus/0Wnfs4CPpTaSU4HZqbiY9YqLiFlBEfEwcBVZMXkeeENu8X+rYioXAAcAR0TEXsBRKa7cOq8Znjsi7gH+RtYx4GPAz6uQpzUBFxGzbkh6p6QLOhqtJQ0ja5e4B1gEHCVpuKQ3AxdVMbU9ya5MnpE0gG3bVrpzNVnbyUsR8ftKJWfNxUXErHubgCOAeyU9T1Y8lgIXRMR8snaGxcBCqtu+8H1gD+AvKadfF9zu52RXUb/Y3opmRckvpTJrDpL2ANYBh0TE8lrnY43BVyJmzePTwAIXEOtLfmLdrAlIWknW8H5SbTOxRuPbWWZmVlrFbmdJGibpTkkPSlom6fwUHyBpfhreYX7HU8DKTJfULmmxpENy+5qU1l8uaVIufqikJWmb6akPvJmZVUnFrkQkDQYGR8T9kvYk68FyEnAasCEipkmaAvSPiAslnQB8BjiBrEfMv0bEEakLYxvQStb3fSFwaEQ8Lek+4DzgXmAuMD0ibuspr3322SdaWlr6/oTNzBrYwoUL/xIRAzvHK9YmEhFrgDVpepOkh8jGEBoHjEmrzQLuAi5M8asjq2r3SNo7FaIxwPyI2AAgaT4wVtJdwF7pISokXU1WpHosIi0tLbS1tfXZeZqZNQNJj3cVr0rvrDRa6cFkVwyDUoEBeBLoGIdoCK8dqmFVivUUX9VFvKvjT5bUJqlt/fr1vTsZMzN7RcWLiKQ3ATcAn00D2r0iXXVUvGU/ImZERGtEtA4cuM3VmJmZlVTRIiJpV7ICck1E3JjCa9Ntqo52k3UpvppsyOsOQ1Osp/jQLuJmZlYlleydJeBK4KGIuCy3aA7Q0cNqEnBzLj4x9dI6Eng23faaBxybXsTTHzgWmJeWbZR0ZDrWxNy+zMysCir5sOE/kA05vUTSohT7EjANmC3pDOBx4JS0bC5Zz6x2srexnQ4QERskfQ1YkNa7tKORHTibbFTVPcga1HtsVDczs77VdA8btra2hntnmZntGEkLI6K1c9xjZ5mZWWkuImZmVpqLiJmZleZRfPtQy5Rbu122ctqJVczEzKw6fCViZmaluYiYmVlpLiJmZlaai4iZmZXmImJmZqW5iJiZWWkuImZmVpqLiJmZleYiYmZmpbmImJlZaS4iZmZWmouImZmV5iJiZmaluYiYmVlpFSsikmZKWidpaS72S0mL0mdlx7vXJbVIejG37Me5bQ6VtERSu6TpkpTiAyTNl7Q8ffev1LmYmVnXKnklchUwNh+IiI9GxKiIGAXcANyYW/xox7KIOCsXvxw4ExiZPh37nALcEREjgTvSvJmZVVHFikhE3A1s6GpZupo4Bbiup31IGgzsFRH3REQAVwMnpcXjgFlpelYubmZmVVKrNpH3AWsjYnkuNkLSA5J+K+l9KTYEWJVbZ1WKAQyKiDVp+klgUHcHkzRZUpuktvXr1/fRKZiZWa2KyAReexWyBhgeEQcDnwOulbRX0Z2lq5ToYfmMiGiNiNaBAweWzdnMzDqp+jvWJb0O+Gfg0I5YRGwGNqfphZIeBd4BrAaG5jYfmmIAayUNjog16bbXumrkb2Zmr6rFlcg/AQ9HxCu3qSQNlNQvTe9H1oC+It2u2ijpyNSOMhG4OW02B5iUpifl4mZmViWV7OJ7HfAn4ABJqySdkRaNZ9sG9aOAxanL738AZ0VER6P82cBPgXbgUeC2FJ8GHCNpOVlhmlapczEzs65V7HZWREzoJn5aF7EbyLr8drV+G3BQF/GngKN7l6WZmfWGn1g3M7PSXETMzKw0FxEzMyut6l18m1XLlFt7XL5y2olVysTMrO/4SsTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0lxEzMysNBcRMzMrzUXEzMxKcxExM7PSXETMzKw0FxEzMyvNRcTMzEpzETEzs9Iq+Y71mZLWSVqai10iabWkRelzQm7ZRZLaJT0i6bhcfGyKtUuakouPkHRviv9S0m6VOhczM+taJa9ErgLGdhH/XkSMSp+5AJIOBMYD707b/EhSP0n9gB8CxwMHAhPSugDfTPt6O/A0cEYFz8XMzLpQsSISEXcDGwquPg64PiI2R8RjQDtwePq0R8SKiPgbcD0wTpKAfwT+I20/CzipL/M3M7Ptq0WbyLmSFqfbXf1TbAjwRG6dVSnWXfwtwDMRsaVTvEuSJktqk9S2fv36vjoPM7OmV+0icjmwPzAKWAN8txoHjYgZEdEaEa0DBw6sxiHNzJpCVd+xHhFrO6YlXQHckmZXA8Nyqw5NMbqJPwXsLel16Wokv76ZmVVJVa9EJA3OzZ4MdPTcmgOMl7S7pBHASOA+YAEwMvXE2o2s8X1ORARwJ/DhtP0k4OZqnIOZmb2qYlcikq4DxgD7SFoFXAyMkTQKCGAl8CmAiFgmaTbwILAFOCcitqb9nAvMA/oBMyNiWTrEhcD1kr4OPABcWalzMTOzrlWsiETEhC7C3f6hj4ipwNQu4nOBuV3EV5D13jIzsxrxE+tmZlbadouIpI9I2jNNf0XSjZIOqXxqZma2sytyJfK/I2KTpNHAP5Hdkrq8smmZmVk9KFJEtqbvE4EZEXEr4HGqzMysUBFZLeknwEeBuZJ2L7idmZk1uCLF4BSyLrbHRcQzwADgC5VMyszM6sN2u/hGxAuS1gGjgeVkz3Esr3Ri9qqWKbf2uHzltBOrlImZ2WsV6Z11MdmDfRel0K7ALyqZlJmZ1Ycit7NOBj4EPA8QEf8F7FnJpMzMrD4UKSJ/S2NVBYCkN1Y2JTMzqxdFisjs1Dtrb0lnAr8BrqhsWmZmVg+KNKx/R9IxwEbgAOCrETG/4pmZmdlOr9AAjKlouHCYmdlrdFtEJG0itYN0XgREROxVsazMzKwudFtEIsI9sMzMrEeFbmelUXtHk12Z/D4iHqhoVmZmVheKPGz4VWAW8BZgH+AqSV+pdGJmZrbzK3Il8j+B90bEXwEkTQMWAV+vYF5mZlYHijwn8l/A63PzuwOrt7eRpJmS1klamot9W9LDkhZLuknS3ineIulFSYvS58e5bQ6VtERSu6TpkpTiAyTNl7Q8ffcveM5mZtZHihSRZ4Flkq6S9DNgKfBM+oM+vYftrgLGdorNBw6KiPcAf+bV8bgAHo2IUelzVi5+OXAmMDJ9OvY5BbgjIkYCd6R5MzOroiK3s25Knw53FdlxRNwtqaVT7Pbc7D3Ah3vah6TBwF4RcU+avxo4CbgNGAeMSavOSnldWCQ3MzPrG0WeWJ9VoWN/Avhlbn6EpAfInoz/SkT8DhgCrMqtsyrFAAZFxJo0/SQwqLsDSZoMTAYYPnx432RvZmaFemd9UNIDkjZI2ihpk6SNvTmopC+TvZfkmhRaAwyPiIOBzwHXSir8MGN+gMhuls+IiNaIaB04cGAvMjczs7wit7O+D/wzsCT9se4VSacBHwSO7thfRGwGNqfphZIeBd5B1oA/NLf5UF5t1F8raXBErEm3vdb1NjczM9sxRRrWnwCW9lEBGQt8EfhQRLyQiw+U1C9N70fWgL4i3a7aKOnI1CtrInBz2mwOMClNT8rFzcysSopciXwRmCvpt6SrBYCIuKynjSRdR9bwvY+kVcDFZL2xdgfmp56696SeWEcBl0p6CXgZOCsiNqRdnU3W02sPsgb121J8Gtkw9WcAj5O9C97MzKqoSBGZCjxH9qzIbkV3HBETughf2c26NwA3dLOsDTioi/hTwNFF8zEzs75XpIjsGxHb/BE3MzMr0iYyV9KxFc/EzMzqTpEi8mng12lYkj7p4mtmZo2hyMOGfq+ImZl1qej7RPqTdbt9ZSDGiLi7UkmZmVl92G4RkfRJ4HyyB/0WAUcCfwL+saKZmZnZTq9Im8j5wGHA4xHxAeBg4JlKJmVmZvWhSBH5a+6FVLtHxMPAAZVNy8zM6kGRNpFV6eVRvyJ70vxpsifEzcysyRXpnXVymrxE0p3Am4FfVzQrMzOrC0WGgt9f0u4ds0AL8IZKJmVmZvWhSJvIDcBWSW8HZgDDgGsrmpWZmdWFIkXk5YjYApwM/FtEfAEYXNm0zMysHhQpIi9JmkD2zo5bUmzXyqVkZmb1okgROR34e2BqRDwmaQTw88qmZWZm9aBI76wHgfNy848B36xkUmZmVh+KXImYmZl1yUXEzMxK67aISPp5+j6/7M4lzZS0TtLSXGyApPmSlqfv/ikuSdMltUtaLOmQ3DaT0vrLJU3KxQ+VtCRtM13pxe1mZlYdPbWJHCppX+ATkq4me9DwFRGxocD+rwJ+AFydi00B7oiIaZKmpPkLgePJhpsfCRwBXA4cIWkAcDHQCgSwUNKciHg6rXMmcC8wFxgL3FYgr4bSMuXWHpevnHZilTIxs2bT0+2sHwN3AO8EFnb6tBXZeXrnSOdiMw6YlaZnASfl4ldH5h5gb0mDgeOA+RGxIRWO+cDYtGyviLgnIoKsUJ2EmZlVTbdFJCKmR8S7gJkRsV9EjMh99uvFMQdFxJo0/SQwKE0PAZ7IrbcqxXqKr+oivg1JkyW1SWpbv359L1I3M7O8Il18Py3pvcD7UujuiFjcFwePiJAUfbGv7RxnBtmQLbS2tlb8eGZmzaLIAIznAdcAb02fayR9phfHXJtuRZG+16X4arJxuToMTbGe4kO7iJuZWZUU6eL7SeCIiPhqRHyV7PW4Z/bimHPIhlAhfd+ci09MvbSOBJ5Nt73mAcdK6p96ch0LzEvLNko6MvXKmpjbl5mZVUGRl1IJ2Jqb30qnnlrdbihdB4wB9pG0iqyX1TRgtqQzyF5udUpafS5wAtAOvEA23AoRsUHS14AFab1Lcz3DzibrAbYHWa+spuuZZWZWS0WKyM+AeyXdlOZPAq4ssvOImNDNoqO7WDeAc7rZz0xgZhfxNuCgIrmYmVnfK9Kwfpmku4DRKXR6RDxQ0azMzKwuFLkSISLuB+6vcC5mZlZnPHaWmZmV5iJiZmal9VhEJPWTdGe1kjEzs/rSYxGJiK3Ay5LeXKV8zMysjhRpWH8OWCJpPvB8RzAizut+k8a0vdFyzcyaTZEicmP6mJmZvUaR50RmSdoDGB4Rj1QhJzMzqxNFBmD878Ai4NdpfpSkORXOy8zM6kCRLr6XAIcDzwBExCKgN+8TMTOzBlGkiLwUEc92ir1ciWTMzKy+FGlYXybpY0A/SSOB84A/VjYtMzOrB0WuRD4DvBvYDFwHbAQ+W8GczMysThTpnfUC8GVJ38xmY1Pl0zIzs3pQpHfWYZKWAIvJHjr8v5IOrXxqZma2syvSJnIlcHZE/A5A0miyF1W9p5KJmZnZzq9Im8jWjgICEBG/B7ZULiUzM6sX3RYRSYdIOgT4raSfSBoj6f2SfgTcVfaAkg6QtCj32Sjps5IukbQ6Fz8ht81FktolPSLpuFx8bIq1S5pSNiczMyunp9tZ3+00f3FuOsoeMA2dMgqyoeaB1cBNwOnA9yLiO/n1JR0IjCfrIbYv8BtJ70iLfwgcA6wCFkiaExEPls3NzMx2TLdFJCI+UIXjHw08GhGPS+punXHA9RGxGXhMUjvZE/QA7RGxAkDS9WldFxEzsyrZbsO6pL2BiUBLfv0+Ggp+PNmzJx3OlTQRaAMuiIingSHAPbl1VqUYwBOd4kd0dRBJk4HJAMOHD++DtM3MDIo1rM8lKyBLgIW5T69I2g34EPDvKXQ5sD/Zra41bHs7rbSImBERrRHROnDgwL7arZlZ0yvSxff1EfG5Chz7eOD+iFgL0PENIOkK4JY0uxoYlttuaIrRQ9zMzKqgyJXIzyWdKWmwpAEdnz449gRyt7IkDc4tOxlYmqbnAOMl7S5pBDASuA9YAIyUNCJd1YxP65qZWZUUuRL5G/Bt4Mu82isr6MVw8JLeSNar6lO58LckjUr7XtmxLCKWSZpN1mC+BTgnvfsdSecC84B+wMyIWFY2JzMz23FFisgFwNsj4i99ddCIeB54S6fYqT2sPxWY2kV8LlmbjZW0vffGr5x2YpUyMbN6VOR2VjvwQqUTMTOz+lPkSuR5YJGkO8mGgwf6rIuvmZnVsSJF5FfpY2Zm9hpF3icyqxqJmJlZ/SnyxPpjdDFWVkSU7p1lZmaNocjtrNbc9OuBjwB98ZyImZnVue32zoqIp3Kf1RHxfcD9Ps3MrNDtrENys7uQXZkUuYIxM7MGV6QY5AdC3EL2NPkpFcnGzMzqSpHeWdV4r4iZmdWhIrezdgf+B9u+T+TSyqVlZmb1oMjtrJuBZ8neIbJ5O+uamVkTKVJEhkbE2IpnYmZmdafIAIx/lPR3Fc/EzMzqTpErkdHAaenJ9c2AgIiI91Q0MzMz2+kVKSLHVzwLMzOrS0W6+D5ejUTMzKz+FGkTMTMz61LNioiklZKWSFokqS3FBkiaL2l5+u6f4pI0XVK7pMX5oVgkTUrrL5c0qVbnY2bWjGp9JfKBiBgVER0jBU8B7oiIkcAdaR6ydpmR6TMZuByyogNcDBwBHA5c3FF4zMys8mpdRDobB3S8BGsWcFIufnVk7gH2ljQYOA6YHxEbIuJpYD7gZ1rMzKqklkUkgNslLZQ0OcUGRcSaNP0kMChNDwGeyG27KsW6i7+GpMmS2iS1rV+/vi/PwcysqdVySPfREbFa0luB+ZIezi+MiJC0zRsVy4iIGcAMgNbW1j7Zp5mZ1fBKJCJWp+91wE1kbRpr020q0ve6tPpqYFhu86Ep1l3czMyqoCZFRNIbJe3ZMQ0cCywF5gAdPawmkQ3+SIpPTL20jgSeTbe95gHHSuqfGtSPTTEzM6uCWt3OGgTcJKkjh2sj4teSFgCzJZ0BPM6rL7+aC5wAtAMvAKcDRMQGSV8DFqT1Lo2IDdU7DTOz5laTIhIRK4D3dhF/Cji6i3gA53Szr5nAzL7O0czMts/vSrcetUy5tcflK6edWKVMzGxntLM9J2JmZnXERcTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0lxEzMysNBcRMzMrzUXEzMxKcxExM7PSXETMzKw0FxEzMyvNRcTMzEpzETEzs9L8PhGrGL+LxKzx+UrEzMxKq3oRkTRM0p2SHpS0TNL5KX6JpNWSFqXPCbltLpLULukRScfl4mNTrF3SlGqfi5lZs6vF7awtwAURcb+kPYGFkuanZd+LiO/kV5Z0IDAeeDewL/AbSe9Ii38IHAOsAhZImhMRD1blLMzMrPpFJCLWAGvS9CZJDwFDethkHHB9RGwGHpPUDhyelrVHxAoASdendV1EzMyqpKZtIpJagIOBe1PoXEmLJc2U1D/FhgBP5DZblWLdxbs6zmRJbZLa1q9f35enYGbW1GpWRCS9CbgB+GxEbAQuB/YHRpFdqXy3r44VETMiojUiWgcOHNhXuzUza3o16eIraVeyAnJNRNwIEBFrc8uvAG5Js6uBYbnNh6YYPcTNzKwKatE7S8CVwEMRcVkuPji32snA0jQ9BxgvaXdJI4CRwH3AAmCkpBGSdiNrfJ9TjXMwM7NMLa5E/gE4FVgiaVGKfQmYIGkUEMBK4FMAEbFM0myyBvMtwDkRsRVA0rnAPKAfMDMillXvNMzMrBa9s34PqItFc3vYZiowtYv43J62s51bT0+0+2l2s/rgJ9bNzKw0FxEzMyvNRcTMzEpzETEzs9JcRMzMrDQXETMzK81FxMzMSnMRMTOz0vx6XKtLfvWu2c7BVyJmZlaai4iZmZXmImJmZqW5iJiZWWluWLeG5BGCzarDVyJmZlaai4iZmZXm21lmnfhWmFlxvhIxM7PS6v5KRNJY4F/J3rP+04iYVuOUrIH5SXmz16rrIiKpH/BD4BhgFbBA0pyIeLC2mZl1zbfKrNHUdREBDgfaI2IFgKTrgXGAi4jVnd5c5Wxv2+3pzb5d/JqbIqLWOZQm6cPA2Ij4ZJo/FTgiIs7ttN5kYHKaPQB4JLd4H+AvVUi3Vnx+9a/Rz7HRzw8a4xzfFhEDOwfr/UqkkIiYAczoapmktohorXJKVePzq3+Nfo6Nfn7Q2OdY772zVgPDcvNDU8zMzKqg3ovIAmCkpBGSdgPGA3NqnJOZWdOo69tZEbFF0rnAPLIuvjMjYtkO7qbL21wNxOdX/xr9HBv9/KCBz7GuG9bNzKy26v12lpmZ1ZCLiJmZlda0RUTSWEmPSGqXNKXW+VSCpJWSlkhaJKmt1vn0lqSZktZJWpqLDZA0X9Ly9N2/ljn2VjfneImk1el3XCTphFrm2BuShkm6U9KDkpZJOj/FG+J37OH8GuY37Kwp20TScCl/JjdcCjCh0YZLkbQSaI2Ien/ICQBJRwHPAVdHxEEp9i1gQ0RMS/8z0D8iLqxlnr3RzTleAjwXEd+pZW59QdJgYHBE3C9pT2AhcBJwGg3wO/ZwfqfQIL9hZ816JfLKcCkR8TegY7gU24lFxN3Ahk7hccCsND2L7B9s3ermHBtGRKyJiPvT9CbgIWAIDfI79nB+DatZi8gQ4Inc/Coa84cO4HZJC9PQL41oUESsSdNPAoNqmUwFnStpcbrdVZe3ejqT1AIcDNxLA/6Onc4PGvA3hOYtIs1idEQcAhwPnJNulTSsyO7NNuL92cuB/YFRwBrguzXNpg9IehNwA/DZiNiYX9YIv2MX59dwv2GHZi0iTTFcSkSsTt/rgJvIbuM1mrXpPnTH/eh1Nc6nz0XE2ojYGhEvA1dQ57+jpF3J/sBeExE3pnDD/I5dnV+j/YZ5zVpEGn64FElvTA17SHojcCywtOet6tIcYFKangTcXMNcKqLjj2tyMnX8O0oScCXwUERcllvUEL9jd+fXSL9hZ03ZOwsgdbH7Pq8OlzK1thn1LUn7kV19QDa8zbX1fo6SrgPGkA2rvRa4GPgVMBsYDjwOnBIRddsw3c05jiG7DRLASuBTufaDuiJpNPA7YAnwcgp/iazdoO5/xx7ObwIN8ht21rRFxMzMeq9Zb2eZmVkfcBExM7PSXETMzKw0FxEzMyvNRcTMzEpzEbGGJum5CuxzVH4U1jRC6+d7sb+PSHpI0p19k2HpPFZK2qeWOVj9cREx23GjgL4cyvsM4MyI+EAf7tOsKlxErGlI+oKkBWkQvH9JsZZ0FXBFev/D7ZL2SMsOS+sukvRtSUvTCAeXAh9N8Y+m3R8o6S5JKySd183xJ6T3uyyV9M0U+yowGrhS0rc7rT9Y0t3pOEslvS/FL5fUlvL9l9z6KyV9I63fJukQSfMkPSrprLTOmLTPW5W9T+fHkrb5OyDp45LuS/v6iaR+6XNVymWJpP/Vy5/EGkFE+ONPw37I3uEA2bAvMwCR/c/TLcBRQAuwBRiV1psNfDxNLwX+Pk1PA5am6dOAH+SOcQnwR2B3sifNnwJ27ZTHvsD/AwaSjSDwf4CT0rK7yN770jn3C4Avp+l+wJ5pekAudhfwnjS/Evh0mv4esBjYMx1zbYqPAf4K7Je2nw98OLf9PsC7gP/sOAfgR8BE4FBgfi6/vWv9+/pT+4+vRKxZHJs+DwD3A+8ERqZlj0XEojS9EGiRtDfZH+0/pfi129n/rRGxObIXgK1j26HMDwPuioj1EbEFuIasiPVkAXB6einV30X2fgqAUyTdn87l3cCBuW06xoBbAtwbEZsiYj2wOZ0TwH2RvUtnK3Ad2ZVQ3tFkBWOBpEVpfj9gBbCfpH+TNBbYiDW919U6AbMqEfCNiPjJa4LZOx8250JbgT1K7L/zPnr9bysi7k7D958IXCXpMrJxmT4PHBYRT0u6Cnh9F3m83Cmnl3M5dR7rqPO8gFkRcVHnnCS9FzgOOIvsbX2f2NHzssbiKxFrFvOAT6T3PCBpiKS3drdyRDwDbJJ0RAqNzy3eRHabaEfcB7xf0j7KXs88AfhtTxtIehvZbagrgJ8ChwB7Ac8Dz0oaRPaumB11eBrBehfgo8DvOy2/A/hwx38fZe8/f1vqubVLRNwAfCXlY03OVyLWFCLidknvAv6UjdbNc8DHya4aunMGcIWkl8n+4D+b4ncCU9Ktnm8UPP4aZe8Ov5Ps//RvjYjtDXc+BviCpJdSvhMj4jFJDwAPk72d8w9Fjt/JAuAHwNtTPjflF0bEg5K+QvZWzF2Al4BzgBeBn+Ua4re5UrHm41F8zboh6U0R8VyangIMjojza5xWr0gaA3w+Ij5Y41SsQfhKxKx7J0q6iOzfyeNkvbLMLMdXImZmVpob1s3MrDQXETMzK81FxMzMSnMRMTOz0lxEzMystP8PpPFMfpeALD4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "text_max_len = 80\n",
    "summary_max_len = 8\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 80 이하인 샘플의 비율: 0.9076679305076113\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 76469\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67730 36460 45593 ... 21723 60228  8842]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what = np.arange(50)\n",
    "what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       "       'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       "       'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       "       'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[[3,2,1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 15293\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 61176\n",
      "훈련 레이블의 개수 : 61176\n",
      "테스트 데이터의 개수 : 15293\n",
      "테스트 레이블의 개수 : 15293\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 38480\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 22413\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 16067\n",
      "단어 집합에서 희귀 단어의 비율: 58.245841995842\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.5004265036474946\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_vocab = 16000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x7f9c98ee2280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/nltk/data.py\", line 1160, in __del__\n",
      "    if not self.closed:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/nltk/data.py\", line 1180, in closed\n",
      "    return self.stream.closed\n",
      "AttributeError: 'SeekableUnicodeStreamReader' object has no attribute 'stream'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 219, 808, 185, 108, 15, 50, 130, 839, 222, 39, 1196, 678, 186, 7, 461, 119, 179, 51, 391, 107], [495, 30, 6, 648, 304, 465, 1, 28, 103, 675, 221, 375, 362, 429, 234, 221, 3219, 4, 650, 13, 308, 143, 2515, 38, 2938, 5, 8351, 280, 5384, 8352, 4099, 40, 382, 308, 634, 675, 173, 11012, 812, 295, 4, 40, 4, 397], [6, 249, 19, 2120, 697, 548, 71, 137, 839, 978, 222, 2669, 5958, 186, 12, 137, 2040, 4604, 5641, 7531, 68, 1380, 321, 554]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 11548\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 7232\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 4316\n",
      "단어 집합에서 희귀 단어의 비율: 62.62556286802909\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.1842131928260207\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 10, 552, 83], [1, 439, 57, 112, 73, 6, 91, 164], [1, 118, 3055, 22, 270], [1, 247, 51, 51, 291, 1758, 91], [1, 25]]\n",
      "target\n",
      "decoder  [[10, 552, 83, 2], [439, 57, 112, 73, 6, 91, 164, 2], [118, 3055, 22, 270, 2], [247, 51, 51, 291, 1758, 91, 2], [25, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 5000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 640\n",
      "삭제할 테스트 데이터의 개수 : 209\n",
      "훈련 데이터의 개수 : 60536\n",
      "훈련 레이블의 개수 : 60536\n",
      "테스트 데이터의 개수 : 15084\n",
      "테스트 레이블의 개수 : 15084\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "# encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "# encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 80, 128)      2048000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 80, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 80, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    640000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 80, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 5000)   1285000     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,812,104\n",
      "Trainable params: 5,812,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 80, 128)      2048000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 80, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 80, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    640000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 80, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 5000)   2565000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,092,360\n",
      "Trainable params: 7,092,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on AdditiveAttention in module keras.layers.dense_attention object:\n",
      "\n",
      "class AdditiveAttention(BaseDenseAttention)\n",
      " |  AdditiveAttention(use_scale=True, **kwargs)\n",
      " |  \n",
      " |  Additive attention layer, a.k.a. Bahdanau-style attention.\n",
      " |  \n",
      " |  Inputs are `query` tensor of shape `[batch_size, Tq, dim]`, `value` tensor of\n",
      " |  shape `[batch_size, Tv, dim]` and `key` tensor of shape\n",
      " |  `[batch_size, Tv, dim]`. The calculation follows the steps:\n",
      " |  \n",
      " |  1. Reshape `query` and `key` into shapes `[batch_size, Tq, 1, dim]`\n",
      " |     and `[batch_size, 1, Tv, dim]` respectively.\n",
      " |  2. Calculate scores with shape `[batch_size, Tq, Tv]` as a non-linear\n",
      " |     sum: `scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`\n",
      " |  3. Use scores to calculate a distribution with shape\n",
      " |     `[batch_size, Tq, Tv]`: `distribution = tf.nn.softmax(scores)`.\n",
      " |  4. Use `distribution` to create a linear combination of `value` with\n",
      " |     shape `[batch_size, Tq, dim]`:\n",
      " |     `return tf.matmul(distribution, value)`.\n",
      " |  \n",
      " |  Args:\n",
      " |    use_scale: If `True`, will create a variable to scale the attention scores.\n",
      " |    causal: Boolean. Set to `True` for decoder self-attention. Adds a mask such\n",
      " |      that position `i` cannot attend to positions `j > i`. This prevents the\n",
      " |      flow of information from the future towards the past.\n",
      " |    dropout: Float between 0 and 1. Fraction of the units to drop for the\n",
      " |      attention scores.\n",
      " |  \n",
      " |  Call Args:\n",
      " |  \n",
      " |    inputs: List of the following tensors:\n",
      " |      * query: Query `Tensor` of shape `[batch_size, Tq, dim]`.\n",
      " |      * value: Value `Tensor` of shape `[batch_size, Tv, dim]`.\n",
      " |      * key: Optional key `Tensor` of shape `[batch_size, Tv, dim]`. If not\n",
      " |        given, will use `value` for both `key` and `value`, which is the\n",
      " |        most common case.\n",
      " |    mask: List of the following tensors:\n",
      " |      * query_mask: A boolean mask `Tensor` of shape `[batch_size, Tq]`.\n",
      " |        If given, the output will be zero at the positions where\n",
      " |        `mask==False`.\n",
      " |      * value_mask: A boolean mask `Tensor` of shape `[batch_size, Tv]`.\n",
      " |        If given, will apply the mask such that values at positions where\n",
      " |        `mask==False` do not contribute to the result.\n",
      " |    training: Python boolean indicating whether the layer should behave in\n",
      " |      training mode (adding dropout) or in inference mode (no dropout).\n",
      " |    return_attention_scores: bool, it `True`, returns the attention scores\n",
      " |      (after masking and softmax) as an additional output argument.\n",
      " |  \n",
      " |  Output:\n",
      " |  \n",
      " |    Attention outputs of shape `[batch_size, Tq, dim]`.\n",
      " |    [Optional] Attention scores after masking and softmax with shape\n",
      " |      `[batch_size, Tq, Tv]`.\n",
      " |  \n",
      " |  The meaning of `query`, `value` and `key` depend on the application. In the\n",
      " |  case of text similarity, for example, `query` is the sequence embeddings of\n",
      " |  the first piece of text and `value` is the sequence embeddings of the second\n",
      " |  piece of text. `key` is usually the same tensor as `value`.\n",
      " |  \n",
      " |  Here is a code example for using `AdditiveAttention` in a CNN+Attention\n",
      " |  network:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Variable-length int sequences.\n",
      " |  query_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
      " |  value_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
      " |  \n",
      " |  # Embedding lookup.\n",
      " |  token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)\n",
      " |  # Query embeddings of shape [batch_size, Tq, dimension].\n",
      " |  query_embeddings = token_embedding(query_input)\n",
      " |  # Value embeddings of shape [batch_size, Tv, dimension].\n",
      " |  value_embeddings = token_embedding(value_input)\n",
      " |  \n",
      " |  # CNN layer.\n",
      " |  cnn_layer = tf.keras.layers.Conv1D(\n",
      " |      filters=100,\n",
      " |      kernel_size=4,\n",
      " |      # Use 'same' padding so outputs have the same shape as inputs.\n",
      " |      padding='same')\n",
      " |  # Query encoding of shape [batch_size, Tq, filters].\n",
      " |  query_seq_encoding = cnn_layer(query_embeddings)\n",
      " |  # Value encoding of shape [batch_size, Tv, filters].\n",
      " |  value_seq_encoding = cnn_layer(value_embeddings)\n",
      " |  \n",
      " |  # Query-value attention of shape [batch_size, Tq, filters].\n",
      " |  query_value_attention_seq = tf.keras.layers.AdditiveAttention()(\n",
      " |      [query_seq_encoding, value_seq_encoding])\n",
      " |  \n",
      " |  # Reduce over the sequence axis to produce encodings of shape\n",
      " |  # [batch_size, filters].\n",
      " |  query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
      " |      query_seq_encoding)\n",
      " |  query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
      " |      query_value_attention_seq)\n",
      " |  \n",
      " |  # Concatenate query and document encodings to produce a DNN input layer.\n",
      " |  input_layer = tf.keras.layers.Concatenate()(\n",
      " |      [query_encoding, query_value_attention])\n",
      " |  \n",
      " |  # Add DNN layers, and create Model.\n",
      " |  # ...\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdditiveAttention\n",
      " |      BaseDenseAttention\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, use_scale=True, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDenseAttention:\n",
      " |  \n",
      " |  call(self, inputs, mask=None, training=None, return_attention_scores=False)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(attn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "237/237 [==============================] - 53s 116ms/step - loss: 3.0250 - val_loss: 2.7256\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 27s 114ms/step - loss: 2.6574 - val_loss: 2.5623\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 26s 111ms/step - loss: 2.5034 - val_loss: 2.4363\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 26s 111ms/step - loss: 2.3777 - val_loss: 2.3502\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 27s 112ms/step - loss: 2.2882 - val_loss: 2.2922\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 27s 112ms/step - loss: 2.2214 - val_loss: 2.2535\n",
      "Epoch 7/50\n",
      "237/237 [==============================] - 26s 111ms/step - loss: 2.1662 - val_loss: 2.2263\n",
      "Epoch 8/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 2.1187 - val_loss: 2.2031\n",
      "Epoch 9/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 2.0742 - val_loss: 2.1850\n",
      "Epoch 10/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 2.0330 - val_loss: 2.1699\n",
      "Epoch 11/50\n",
      "237/237 [==============================] - 27s 112ms/step - loss: 1.9946 - val_loss: 2.1574\n",
      "Epoch 12/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 1.9591 - val_loss: 2.1530\n",
      "Epoch 13/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 1.9264 - val_loss: 2.1430\n",
      "Epoch 14/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 1.8956 - val_loss: 2.1409\n",
      "Epoch 15/50\n",
      "237/237 [==============================] - 26s 112ms/step - loss: 1.8665 - val_loss: 2.1418\n",
      "Epoch 16/50\n",
      "237/237 [==============================] - 27s 112ms/step - loss: 1.8389 - val_loss: 2.1435\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f9c52fd3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 545, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1263, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : bought iams kitten formula cheapest pet store kitten likes give diarrhea anything smell room much qualifies good kitten food book \n",
      "실제 요약 : kitten likes it \n",
      "예측 요약 :  not for my dog\n",
      "\n",
      "\n",
      "원문 : got paid complaints buying food storage light easily stored perfect \n",
      "실제 요약 : as good as advertised \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : like using various types sauce foods tacos taco salads hamburgers etc first time used chipotle sauce loved smoking flavor perfect tacos fixed daughter tried liked well thick enough thick something ate hands flavor good sauce hot like taste basically good middle road sauce far heat factor providing great taste anyone wants something little different \n",
      "실제 요약 : my new favorite sauce \n",
      "예측 요약 :  great flavor\n",
      "\n",
      "\n",
      "원문 : recently received one bars voxbox review purposes really quite enjoyed banana person like almonds figured would test glad would picked box grocery store since tried one purchased three boxes prefer microwave seconds since bar tastes better heated also bit crunchy overpowering flavour size perfect carry around purse keep car travel bar also contains traces coconut pecans like either cannot even taste ingredients \n",
      "실제 요약 : delicious snack \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : best price seen anywhere good start soy son satisfied getting extra nutrients growing age \n",
      "실제 요약 : great price \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : problems received order specified time great product give two thumbs would definitely order \n",
      "실제 요약 : great rice \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : tasted watery used smallest amount water use cup oz chocolate flavorless mostly super weak recommend \n",
      "실제 요약 : water \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : big hot tea drinker always looking new kinds try high hopes tea could stomach course hibiscus tea compare guess cup tea \n",
      "실제 요약 : wanted to like it \n",
      "예측 요약 :  not the best\n",
      "\n",
      "\n",
      "원문 : dog treat eater turns nose treats organic however literally bounce way run like crazy get one softer consistency chicken breasts stick like dried treats \n",
      "실제 요약 : my dog loves these \n",
      "예측 요약 :  my dog loves these\n",
      "\n",
      "\n",
      "원문 : thats exactly want darker thats one things makes special right fortunate enough specialty store st carries variety blue ridge jams habanero peach habanero pineapple hooked great cracker cream cheese make pineapple best thing ever pears wow reminds preserves grandmother mom made packed fruit sweet enough wonderful velvety good preserves reserve strictly buttered biscuits croissants toast sure dozen uses small company quality creativity go info high photos entire product line \n",
      "실제 요약 : see that color \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : cereal artificial sweetners high fiber great taste \n",
      "실제 요약 : great taste \n",
      "예측 요약 :  great cereal\n",
      "\n",
      "\n",
      "원문 : product difficult find area year thrilled see amazon subscribe save \n",
      "실제 요약 : hard to find \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : feel cold coming product works get rest need traditional medicinals wonderful products work proclaimed \n",
      "실제 요약 : great for sleep \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : liked cookie crisp cereal kid probably love back nature chocolate delight granola adult yes thought eating chocolate breakfast might make people stomachs turn product anything unappetizing tasty sweet chocolate chunks accent dominate granola quite fortunately heavily sweetened otherwise fairly filling leave oily feeling mouth granolas folks prefer really toasty oats granola might little disappointed crunchiest cereal breakfast aisle yummy healthy breakfast maybe even better snack granola worth try \n",
      "실제 요약 : chocolate granola good \n",
      "예측 요약 :  tasty but not too sweet\n",
      "\n",
      "\n",
      "원문 : love nutty flavour consistency always cooks perfection even kids love good great source fiber well \n",
      "실제 요약 : love it \n",
      "예측 요약 :  great oatmeal\n",
      "\n",
      "\n",
      "원문 : product fantastic glad amazon cause cannot find anywhere stores \n",
      "실제 요약 : rasberry tea \n",
      "예측 요약 :  great\n",
      "\n",
      "\n",
      "원문 : taste expected earthy grain somewhat nutty goes well many different vegetables meats quality looks good gets \n",
      "실제 요약 : nice taste and texture \n",
      "예측 요약 :  good\n",
      "\n",
      "\n",
      "원문 : taco bell always tasty treat healthy snack choice chipotle sauce taco bell definitley tasty tried homemade taco great also tried turkey sandwich added much needed zing however also added fat calories like adding half mcdonald quarter pounder taco terms fat content said would eat sauce every day would try otherwise bland food choice probably workout day truly bursting flavor diet nice change pace \n",
      "실제 요약 : good once in while \n",
      "예측 요약 :  tasty and healthy\n",
      "\n",
      "\n",
      "원문 : far best microwave popcorn ever kinda hard find store got amazon enough popcorn last months happy product \n",
      "실제 요약 : you gotta get some of this \n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : wish could afford buy salmon time price little steep every day use extremely good salmon would highly recommend times want serve salmon straight cold salad eating greedily first sample taste skinless boneless would make attractive cannot face items opening salmon flavor outstanding continue use generic cans salmon patties reserve precious ones want salmon form \n",
      "실제 요약 : very good \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : soft delicious keebler brand cookie hard find supermarket generally around bag never sale picked order bags warehouse deals fulfilled amazon good bargain considering much grocery store bag hard find usually variety sale right highly recommend try buy \n",
      "실제 요약 : keebler soft batch chocolate chip cookies ounce \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : brewed coffee times buying bag unfortunately many decaf coffees full flavor regular coffee giving away keep looking anyone tried decafs found one tastes great please let us know \n",
      "실제 요약 : tastes pretty mediocre to me \n",
      "예측 요약 :  not the best\n",
      "\n",
      "\n",
      "원문 : love product one cans leaked contents box horrible smell mess clean \n",
      "실제 요약 : great product \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : summertime lots days degrees central california vita coco pure coconut water comes rescue refreshing drink ever added vitamin takes care nutritional requirement time also vita coco tastes much better coconut waters tried \n",
      "실제 요약 : refreshing and nutritious \n",
      "예측 요약 :  coconut water\n",
      "\n",
      "\n",
      "원문 : best taste pasta cooks well slightly less time typical mass produced pasta nice color mouthfeel excellent \n",
      "실제 요약 : wow excellent \n",
      "예측 요약 :  best pasta ever\n",
      "\n",
      "\n",
      "원문 : large packaging means eat fast flavor wise okay kind salty texture wise nice snappy outside mushy middle \n",
      "실제 요약 : they are okay \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : tea unreal smells great tastes never need add sweetener sugar perfectly sweetened delicious always must household \n",
      "실제 요약 : this is unreal tea \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : love banana taffy found stick wrapper getting much eat would buy taffy \n",
      "실제 요약 : taffy \n",
      "예측 요약 :  stale\n",
      "\n",
      "\n",
      "원문 : volumizing shampoo probably used conditioner clear product line even without separate conditioner nice product sure five star shampoo would consist giving four stars think fair say average sure \n",
      "실제 요약 : basically nice shampoo \n",
      "예측 요약 :  great shampoo\n",
      "\n",
      "\n",
      "원문 : love cofee europe use lot buy see difference happy find brand amazon thank much \n",
      "실제 요약 : cofee \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : girls greenies bedtime lb border collies give teenie greenies sometimes two go nuts \n",
      "실제 요약 : good greenies \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : love cookie mix add handful pecans like gluten filled pecan used eat easy make good \n",
      "실제 요약 : good cookies \n",
      "예측 요약 :  great cookies\n",
      "\n",
      "\n",
      "원문 : delicious good chewy cereal satisfying even kids love multi whole grain almonds hazel nuts raisins natural sweetness encourage try cereal go back overly sweet non american grocery store type cereal \n",
      "실제 요약 : cereal no sugar added ounce boxes \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : always loved popcorn could find discovered found perfect place get shipped time fresh delicious \n",
      "실제 요약 : perfect and delcious \n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : discovered pop chips yet know missing low calorie high flavor great kids lunches need single servings eat whole big bag \n",
      "실제 요약 : pop chips on the go \n",
      "예측 요약 :  great chips\n",
      "\n",
      "\n",
      "원문 : months ago first butternut squash soup hand made fancy restaurant like canned soups saw local grocers remembering soup restaurant decided try delicious favorite lunch go added shredded roasted turkey make heartier taste wonderful order case amazon cheaper local store also tried french onion soup really good avoided others simply best form like chicken noodles tomato soups like tried really made something canned soups would recommend \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  great for quick meal\n",
      "\n",
      "\n",
      "원문 : sell stores near longer mix jiffy golden yellow cake mix box jiffy corn bread result divine cake cornbread prepare directed box combine yum course amazon prime membership guaranteed quick delivery thanks always need amazon com \n",
      "실제 요약 : love jiffy golden yellow cake mix \n",
      "예측 요약 :  wow\n",
      "\n",
      "\n",
      "원문 : absolutely love flavor hamburger helper saw store bought two boxes incase like sorry buy whole case find stores town \n",
      "실제 요약 : love this flavor \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : enjoyed coffee got rather bold flavor little sweeter coffees made stand looking try something new give flavorful hearty blend try \n",
      "실제 요약 : good coffee to start off your day \n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : cant say enought gluten free bisquick husband three kids allergic wheat gluten free diet house tried every product make pancakes waffles biscuits even chicken fingers pleasantly suprised little box wonderful pancakes wonderful waffles great chicken fingers best ive made biscuits delish cant ask anything better wish could buy ina bigger box little cheaper huge fan \n",
      "실제 요약 : gluten free for years best mix ive tried \n",
      "예측 요약 :  great gluten free baking\n",
      "\n",
      "\n",
      "원문 : born argentina top line dulce de leche eat toasts add cakes note noticed dulce de leche acquired taste unless grew eating may consider dulce de leche little sweet \n",
      "실제 요약 : yummy milk spread \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : love afraid became addicted found taste little strong beginning forgot teas home \n",
      "실제 요약 : fantastic tea \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : dog minutes chewed knot went chewing rubber nozzle dog mo old weighed lbs loved idea disappointed spent toy lasted less hour \n",
      "실제 요약 : neat idea but not good for strong chewers \n",
      "예측 요약 :  not for the dog\n",
      "\n",
      "\n",
      "원문 : like fig newtons missing since cut white flour enriched flour diet made whole grain taste good reason cannot give five stars even though advertisement list high fructose corn syrup ingredient actual nutrition label ingredient try food getting difficult find products contain \n",
      "실제 요약 : good product \n",
      "예측 요약 :  contains ingredients\n",
      "\n",
      "\n",
      "원문 : really really tasty much healthier almost every potato chip variety flavors consider huge sampler sea salt vinegar crazy sour personally think vinegar hard stop one serving take sign good snack \n",
      "실제 요약 : new favorite munchies found \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : doctor wanted baby iron fortified cereal son refused eat cereals like earth best gerber etc one eats product made spain seems high quality child fussy may work amazon would sell variety pack right order type even though really boxed together \n",
      "실제 요약 : only cereal my baby will eat \n",
      "예측 요약 :  baby loves it\n",
      "\n",
      "\n",
      "원문 : first pod coffeemaker senseo relegated garage limited coffee options weak tasting coffee bought keurig based amazon reviews liked first love found dark deeply satisfying roast makes day full coffeehouse flavor bitter aftertaste one three featured varieties office timothy breakfast blend coffee people black tiger tully italian roast three subscription hit staff client partners btw personal choice almost always one \n",
      "실제 요약 : this is the reason to own keurig \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : always loved chips buy often grocery store since amazon offered good price purchased case however upon receipt disappointed immediately notified company good ones grocery store wondered old expiration date indicates fine dislike though love store ones decided keep salsa fix anything however purchase bulk amazon biggest disappointment company never responded email \n",
      "실제 요약 : disappointed \n",
      "예측 요약 :  great product but not the best\n",
      "\n",
      "\n",
      "원문 : opened bag like smelling fresh strawberries big sweets right taste like artificial flavoring haribo definitely makes best gummy candies \n",
      "실제 요약 : just like childhood \n",
      "예측 요약 :  yummy\n",
      "\n",
      "\n",
      "원문 : good price tastes great never tried wake roast good flavor price way better local grocery store \n",
      "실제 요약 : maxwell house wake up roast \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\n",
      "\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\n",
      "\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\n",
      "\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\n",
      "\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5\n",
      "2.6.0\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import nltk\n",
    "import tensorflow\n",
    "import summa\n",
    "\n",
    "print(nltk.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(version('summa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26407</th>\n",
       "      <td>Pumps in Thai cave failed just after last boy ...</td>\n",
       "      <td>The water pumps draining the flooded cave in T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47358</th>\n",
       "      <td>Shoaib Akhtar named brand ambassador of Pak Cr...</td>\n",
       "      <td>Former Pakistani fast bowler Shoaib Akhtar has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35085</th>\n",
       "      <td>2 wickets in 2 balls happen twice in the same ...</td>\n",
       "      <td>During the MI-RCB IPL match on Tuesday, two wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14139</th>\n",
       "      <td>WhatsApp sets up system to store payments data...</td>\n",
       "      <td>Facebook-owned messaging service WhatsApp has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22052</th>\n",
       "      <td>Bajaj Electricals MD Anant Bajaj passes away a...</td>\n",
       "      <td>The Managing Director (MD) of Bajaj Electrical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42296</th>\n",
       "      <td>Russia supplying arms to the Taliban: US</td>\n",
       "      <td>The Commander of US forces in Afghanistan, Gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62012</th>\n",
       "      <td>Bitcoin not to be used for payments for now: R...</td>\n",
       "      <td>Executive Director of Reserve Bank of India S ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27758</th>\n",
       "      <td>Instagram may let users ask questions in Stories</td>\n",
       "      <td>Facebook-owned photo-sharing app Instagram is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45404</th>\n",
       "      <td>Who are the nominees for Best Actor at Oscars ...</td>\n",
       "      <td>Denzel Washington is among the nominees for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18323</th>\n",
       "      <td>BJP govt is India's most non-performing asset:...</td>\n",
       "      <td>Ahead of the release of his new book, senior C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "26407  Pumps in Thai cave failed just after last boy ...   \n",
       "47358  Shoaib Akhtar named brand ambassador of Pak Cr...   \n",
       "35085  2 wickets in 2 balls happen twice in the same ...   \n",
       "14139  WhatsApp sets up system to store payments data...   \n",
       "22052  Bajaj Electricals MD Anant Bajaj passes away a...   \n",
       "42296           Russia supplying arms to the Taliban: US   \n",
       "62012  Bitcoin not to be used for payments for now: R...   \n",
       "27758   Instagram may let users ask questions in Stories   \n",
       "45404  Who are the nominees for Best Actor at Oscars ...   \n",
       "18323  BJP govt is India's most non-performing asset:...   \n",
       "\n",
       "                                                    text  \n",
       "26407  The water pumps draining the flooded cave in T...  \n",
       "47358  Former Pakistani fast bowler Shoaib Akhtar has...  \n",
       "35085  During the MI-RCB IPL match on Tuesday, two wi...  \n",
       "14139  Facebook-owned messaging service WhatsApp has ...  \n",
       "22052  The Managing Director (MD) of Bajaj Electrical...  \n",
       "42296  The Commander of US forces in Afghanistan, Gen...  \n",
       "62012  Executive Director of Reserve Bank of India S ...  \n",
       "27758  Facebook-owned photo-sharing app Instagram is ...  \n",
       "45404  Denzel Washington is among the nominees for th...  \n",
       "18323  Ahead of the release of his new book, senior C...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "092767183cb03da6ed3f313548119bc9426a350fa7be41c2565ca0e1bc927587"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
